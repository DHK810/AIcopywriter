{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lab-12-6-seq-to-seq-with-attention-keras-eager.ipynb","provenance":[{"file_id":"1C4fpM7_7IL8ZzF7Gc5abywqQjeQNS2-U","timestamp":1527858391290},{"file_id":"1pExo6aUuw0S6MISFWoinfJv0Ftm9V4qv","timestamp":1527776041613}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H_N4DHnVrseI","colab_type":"text"},"source":["# lab-12-6 sequence to sequence with attention (Keras + eager version)\n","\n","### simple neural machine translation training\n","\n","* sequence to sequence\n","  \n","### Reference\n","* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n","* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n","* [Neural Machine Translation with Attention from Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tnxXKDjq3jEL","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","# Import TensorFlow >= 1.10 and enable eager execution\n","import tensorflow as tf\n","\n","tf.enable_eager_execution()\n","\n","from matplotlib import font_manager, rc\n","\n","rc('font', family='AppleGothic') #for mac\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from pprint import pprint\n","import numpy as np\n","import os\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhZ7IanwrseN","colab_type":"code","colab":{}},"source":["sources = [['I', 'feel', 'hungry'],\n","     ['tensorflow', 'is', 'very', 'difficult'],\n","     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n","     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n","targets = [['나는', '배가', '고프다'],\n","           ['텐서플로우는', '매우', '어렵다'],\n","           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n","           ['텐서플로우는', '매우', '빠르게', '변화한다']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcumEZoyrseP","colab_type":"code","colab":{}},"source":["# vocabulary for sources\n","s_vocab = list(set(sum(sources, [])))\n","s_vocab.sort()\n","s_vocab = ['<pad>'] + s_vocab\n","source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n","idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n","\n","pprint(source2idx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lXB-VUBMrseS","colab_type":"code","colab":{}},"source":["# vocabulary for targets\n","t_vocab = list(set(sum(targets, [])))\n","t_vocab.sort()\n","t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n","target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n","idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n","\n","pprint(target2idx)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"41vsoKVMrseU","colab_type":"code","colab":{}},"source":["def preprocess(sequences, max_len, dic, mode = 'source'):\n","    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n","    \n","    if mode == 'source':\n","        # preprocessing for source (encoder)\n","        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n","        s_len = list(map(lambda sentence : len(sentence), s_input))\n","        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n","        return s_len, s_input\n","    \n","    elif mode == 'target':\n","        # preprocessing for target (decoder)\n","        # input\n","        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n","        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n","        t_len = list(map(lambda sentence : len(sentence), t_input))\n","        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n","        \n","        # output\n","        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n","        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n","        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n","        \n","        return t_len, t_input, t_output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"va-4ia1Cr0O_","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Pnjqa9GOrseW","colab_type":"code","colab":{}},"source":["# preprocessing for source\n","s_max_len = 10\n","s_len, s_input = preprocess(sequences = sources,\n","                            max_len = s_max_len, dic = source2idx, mode = 'source')\n","print(s_len, s_input)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1c38GXgUrseY","colab_type":"code","colab":{}},"source":["# preprocessing for target\n","t_max_len = 12\n","t_len, t_input, t_output = preprocess(sequences = targets,\n","                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n","print(t_len, t_input, t_output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KfNB1D9nrseb","colab_type":"text"},"source":["# hyper-param"]},{"cell_type":"code","metadata":{"id":"aDsB9Jm-rseb","colab_type":"code","colab":{}},"source":["# hyper-parameters\n","epochs = 100\n","batch_size = 4\n","learning_rate = .005\n","total_step = epochs / batch_size\n","buffer_size = 100\n","n_batch = buffer_size//batch_size\n","embedding_dim = 32\n","units = 128\n","\n","# input\n","data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n","data = data.shuffle(buffer_size = buffer_size)\n","data = data.batch(batch_size = batch_size)\n","# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1zeZsExSrsee","colab_type":"code","colab":{}},"source":["def gru(units):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","    if tf.test.is_gpu_available():\n","        return tf.keras.layers.CuDNNGRU(units, \n","                                        return_sequences=True, \n","                                        return_state=True, \n","                                        recurrent_initializer='glorot_uniform')\n","    else:\n","        return tf.keras.layers.GRU(units, \n","                                   return_sequences=True, \n","                                   return_state=True, \n","                                   recurrent_activation='sigmoid', \n","                                   recurrent_initializer='glorot_uniform')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjZ-wsk9rseg","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.enc_units)\n","        \n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)\n","#         print(\"state: {}\".format(state.shape))\n","#         print(\"output: {}\".format(state.shape))\n","              \n","        return output, state\n","    \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1qCbpeersei","colab_type":"code","colab":{}},"source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.dec_units)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        \n","        # used for attention\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, hidden, enc_output):\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        \n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        # * `score = FC(tanh(FC(EO) + FC(H)))`\n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n","        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n","                \n","        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        # * `merged vector = concat(embedding output, context vector)`\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","        \n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","        \n","        return x, state, attention_weights\n","        \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"buk-GeAVrsek","colab_type":"code","colab":{}},"source":["encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n","decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n","\n","def loss_function(real, pred):\n","    mask = 1 - np.equal(real, 0)\n","    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n","    \n","#     print(\"real: {}\".format(real))\n","#     print(\"pred: {}\".format(pred))\n","#     print(\"mask: {}\".format(mask))\n","#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n","    \n","    return tf.reduce_mean(loss_)\n","\n","# creating optimizer\n","optimizer = tf.train.AdamOptimizer()\n","\n","# creating check point (Object-based saving)\n","checkpoint_dir = './data_out/training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                encoder=encoder,\n","                                decoder=decoder)\n","\n","# create writer for tensorboard\n","summary_writer = tf.contrib.summary.create_file_writer(logdir=checkpoint_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bS2gj0Oirsem","colab_type":"code","colab":{}},"source":["EPOCHS = 100\n","\n","for epoch in range(EPOCHS):\n","    \n","    hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    \n","    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n","        loss = 0\n","        with tf.GradientTape() as tape:\n","            enc_output, enc_hidden = encoder(s_input, hidden)\n","            \n","            dec_hidden = enc_hidden\n","            \n","            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n","            \n","            #Teacher Forcing: feeding the target as the next input\n","            for t in range(1, t_input.shape[1]):\n","                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","                \n","                loss += loss_function(t_input[:, t], predictions)\n","            \n","                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n","                \n","        batch_loss = (loss / int(t_input.shape[1]))\n","        \n","        total_loss += batch_loss\n","        \n","        variables = encoder.variables + decoder.variables\n","        \n","        gradient = tape.gradient(loss, variables)\n","        \n","        optimizer.apply_gradients(zip(gradient, variables))\n","        \n","    if epoch % 10 == 0:\n","        #save model every 10 epoch\n","        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n","                                            total_loss / n_batch,\n","                                            batch_loss.numpy()))\n","        checkpoint.save(file_prefix = checkpoint_prefix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FpYBEm2orseo","colab_type":"code","colab":{}},"source":["def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    \n","#     sentence = preprocess_sentence(sentence)\n","\n","    inputs = [inp_lang[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n","        \n","        # storing the attention weigths to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += idx2target[predicted_id] + ' '\n","\n","        if idx2target.get(predicted_id) == '<eos>':\n","            return result, sentence, attention_plot\n","        \n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result, sentence, attention_plot\n","\n","# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n","#                                             s_max_len, t_max_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PnlRSAHlrseq","colab_type":"code","colab":{}},"source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap='viridis')\n","    \n","    fontdict = {'fontsize': 14}\n","    \n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8J_oZ50Qrset","colab_type":"code","colab":{}},"source":["\n","def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n","    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n","        \n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(result))\n","    \n","    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1Al997Hrsev","colab_type":"code","colab":{}},"source":["#restore checkpoint\n","\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"auBLsZAKrsex","colab_type":"code","colab":{}},"source":["sentence = 'I feel hungry'\n","# sentence = 'tensorflow is a framework for deep learning'\n","\n","translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCLI96KVrse0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}