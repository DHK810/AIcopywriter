{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_N4DHnVrseI"
   },
   "source": [
    "# lab-12-6 sequence to sequence with attention (Keras + eager version)\n",
    "\n",
    "### simple neural machine translation training\n",
    "\n",
    "* sequence to sequence\n",
    "  \n",
    "### Reference\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n",
    "* [Neural Machine Translation with Attention from Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhZ7IanwrseN"
   },
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "     ['tensorflow', 'is', 'very', 'difficult'],\n",
    "     ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "     ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "           ['텐서플로우는', '매우', '어렵다'],\n",
    "           ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "           ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcumEZoyrseP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n",
    "\n",
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXB-VUBMrseS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n",
    "\n",
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41vsoKVMrseU"
   },
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # input\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "va-4ia1Cr0O_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pnjqa9GOrseW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5] [[ 1  7 10  0  0  0  0  0  0  0]\n",
      " [13 11 14  5  0  0  0  0  0  0]\n",
      " [13 11  2  9  8  4 12  0  0  0]\n",
      " [13 11 14  6  3  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for source\n",
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n",
    "print(s_len, s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1c38GXgUrseY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6] [[ 1  4  7  3  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  6 10  2  0  0  0  0  0  0  0]\n",
      " [ 1 12  5 11 13  2  0  0  0  0  0  0]\n",
      " [ 1 12  6  9  8  2  0  0  0  0  0  0]] [[ 4  7  3  2  0  0  0  0  0  0  0  0]\n",
      " [12  6 10  2  0  0  0  0  0  0  0  0]\n",
      " [12  5 11 13  2  0  0  0  0  0  0  0]\n",
      " [12  6  9  8  2  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for target\n",
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n",
    "print(t_len, t_input, t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfNB1D9nrseb"
   },
   "source": [
    "# hyper-param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDsB9Jm-rseb"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 128\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size)\n",
    "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zeZsExSrsee"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjZ-wsk9rseg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1qCbpeersei"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buk-GeAVrsek"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "summary_writer = tf.contrib.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bS2gj0Oirsem"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)\n",
    "            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpYBEm2orseo"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "#     sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PnlRSAHlrseq"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J_oZ50Qrset"
   },
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1Al997Hrsev"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x15fb5651fd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restore checkpoint\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auBLsZAKrsex",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensorflow is very difficult\n",
      "Predicted translation: 텐서플로우는 매우 어렵다 <eos> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence = 'tensorflow is very difficult'\n",
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "\n",
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCLI96KVrse0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab-12-6-seq-to-seq-with-attention-keras-eager.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1C4fpM7_7IL8ZzF7Gc5abywqQjeQNS2-U",
     "timestamp": 1527858391290
    },
    {
     "file_id": "1pExo6aUuw0S6MISFWoinfJv0Ftm9V4qv",
     "timestamp": 1527776041613
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
