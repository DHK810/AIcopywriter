{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H_N4DHnVrseI"
   },
   "source": [
    "# lab-12-6 sequence to sequence with attention (Keras + eager version)\n",
    "\n",
    "### simple neural machine translation training\n",
    "\n",
    "* sequence to sequence\n",
    "  \n",
    "### Reference\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\n",
    "* [Neural Machine Translation with Attention from Tensorflow](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# Import TensorFlow >= 1.15 and enable eager execution\n",
    "import tensorflow as tf \n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "rc('font', family='AppleGothic') #for mac\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import random\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#path = '/content/drive/My Drive/Colab Notebooks/moviedata2.json'\n",
    "df = pd.read_json('movies_a.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125539"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = movie_dataframe\n",
    "def preprocessed_df(df):\n",
    "    df.movie_name = df.movie_name.apply(lambda x: x.replace('혻',''))\n",
    "\n",
    "    df.movie_summary = df.movie_summary.apply(lambda x: str(x).lower())\n",
    "    df.movie_summary = df.movie_summary.apply(lambda x: re.sub(\"[^A-Za-z. ]\", '', x))\n",
    "    df.movie_summary = df.movie_summary.apply(lambda x: x.replace('\\\\n',''))\n",
    "\n",
    "    df.movie_tagline = df.movie_tagline.apply(lambda x: str(x).lower())\n",
    "    df.movie_tagline = df.movie_tagline.apply(lambda x: x.replace('\\\\n','').replace('\\\\xa0',''))\n",
    "    df.movie_tagline = df.movie_tagline.apply(lambda x : re.sub('[^A-Za-z. ]', '', x))\n",
    "    df.movie_tagline = df.movie_tagline.apply(lambda x : re.sub(' {2,}', '', x))\n",
    "\n",
    "    tl = list(df[\"movie_tagline\"])\n",
    "    tl = [\"\".join([ j for j in i if j != \"\\n\"]).strip() for i in tl]\n",
    "\n",
    "    df.movie_tagline = tl\n",
    "\n",
    "    #태그라인 없는 데이터 row 삭제\n",
    "    index = df[df['movie_tagline']==''].index\n",
    "    df.drop(index, inplace = True)\n",
    "\n",
    "    #summary 없는 데이터 row 삭제\n",
    "    summary_index = df[df['movie_summary']==''].index\n",
    "    df.drop(summary_index, inplace = True)\n",
    "    \n",
    "    df.movie_summary = df.movie_summary.apply(lambda x: x.split('.'))\n",
    "    df = df.reset_index()\n",
    "    return df.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataframe(df):\n",
    "    df1 = []\n",
    "    for idx,val in enumerate(df.movie_summary):\n",
    "        for val2 in val:\n",
    "            df1.append([df['movie_name'][idx], val2, df['movie_tagline'][idx]])\n",
    "    df1 = pd.DataFrame(df1)\n",
    "    df1.columns = ['movie_name', 'movie_summary', 'movie_tagline']\n",
    "    #summary 없는 데이터 row 삭제\n",
    "    summary_index = df1[df1['movie_summary']==''].index\n",
    "    df1.drop(summary_index, inplace = True)\n",
    "    \n",
    "    return df1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_split(df):\n",
    "  \n",
    "    #단어 3개 미만 문장은 삭제\n",
    "    df_split = []\n",
    "    for idx, val in enumerate(df['movie_summary']):\n",
    "        if len(val)> 3:\n",
    "            df_split.append([df['movie_name'][idx], val, df['movie_tagline'][idx]])\n",
    "    df_split = pd.DataFrame(df_split)\n",
    "    df_split.columns = ['movie_name', 'movie_summary', 'movie_tagline']\n",
    "    #문장 쪼개기\n",
    "    df_split['split'] = \"\"\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for idx, i in enumerate(df_split.movie_summary):\n",
    "        word_tokens = word_tokenize(i)\n",
    "        result = []\n",
    "        for w in word_tokens: \n",
    "            if w not in stop_words: \n",
    "                result.append(w) \n",
    "        df_split['split'][idx] = list(result)\n",
    "        if idx % 1000 == 0:\n",
    "            print(idx)\n",
    "    return df_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_summary</th>\n",
       "      <th>movie_tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joker</td>\n",
       "      <td>[in gotham city mentallytroubled comedian arth...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rambo: Last Blood</td>\n",
       "      <td>[rambo must confront his past and unearth his ...</td>\n",
       "      <td>they drew first blood. he will draw last.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stuber</td>\n",
       "      <td>[a detective recruits his uber driver into an ...</td>\n",
       "      <td>buckle up for aride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>John Wick: Chapter 3 - Parabellum</td>\n",
       "      <td>[john wick is on the run after killing a membe...</td>\n",
       "      <td>on may th every action has consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knives Out</td>\n",
       "      <td>[a detective investigates the death of a patri...</td>\n",
       "      <td>everyone has a motive. no one has a clue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Crawl</td>\n",
       "      <td>[a young woman while attempting to save her fa...</td>\n",
       "      <td>they were here first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Maleficent: Mistress of Evil</td>\n",
       "      <td>[maleficent and her goddaughter aurora begin t...</td>\n",
       "      <td>on octobergo beyond the fairy tale.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>[the early life and career of vito corleone in...</td>\n",
       "      <td>all the power on earth cant change destiny.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>[a meek hobbit from the shire and eight compan...</td>\n",
       "      <td>its power corrupts all who desire it. only one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Star Wars: Episode I - The Phantom Menace</td>\n",
       "      <td>[two jedi escape a hostile blockade to find al...</td>\n",
       "      <td>one truth one hate.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fantastic Beasts: The Crimes of Grindelwald</td>\n",
       "      <td>[the second installment of the fantastic beast...</td>\n",
       "      <td>the fate of one will change the future of all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ghost</td>\n",
       "      <td>[after a young man is murdered his spirit stay...</td>\n",
       "      <td>before sam was murdered he told molly hed love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The Outsiders</td>\n",
       "      <td>[the rivalry between two gangs the poor grease...</td>\n",
       "      <td>s.e. hintons classic novel about youth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The Aftermath</td>\n",
       "      <td>[post world war ii a british colonel and his w...</td>\n",
       "      <td>in the aftermath of war the last thing she exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kong: Skull Island</td>\n",
       "      <td>[after the vietnam war a team of scientists ex...</td>\n",
       "      <td>all hail the king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Cast Away</td>\n",
       "      <td>[a fedex executive undergoes a physical and em...</td>\n",
       "      <td>at the edge of the world his journey begins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Fantastic Beasts and Where to Find Them</td>\n",
       "      <td>[the adventures of writer newt scamander in ne...</td>\n",
       "      <td>what would you do if your beasts escaped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Time Trap</td>\n",
       "      <td>[a professor enters a cave and goes missing,  ...</td>\n",
       "      <td>escape to the future.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Goosebumps</td>\n",
       "      <td>[a teenager teams up with the daughter of youn...</td>\n",
       "      <td>you will believe in monsters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Clash of the Titans</td>\n",
       "      <td>[perseus must battle medusa and the kraken to ...</td>\n",
       "      <td>experience the fantastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The Huntsman: Winter's War</td>\n",
       "      <td>[eric and fellow warrior sara raised as member...</td>\n",
       "      <td>discover the story before snow white.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Grinch</td>\n",
       "      <td>[a grumpy grinch plots to ruin christmas for t...</td>\n",
       "      <td>christmas is canceled.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Dumbo</td>\n",
       "      <td>[a young elephant whose oversized ears enable ...</td>\n",
       "      <td>ina beloved tale will take you to new heights.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Clash of the Titans</td>\n",
       "      <td>[perseus demigod son of zeus battles the minio...</td>\n",
       "      <td>the clash begins in d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Spectre</td>\n",
       "      <td>[a cryptic message from s past sends him pitte...</td>\n",
       "      <td>the dead are alive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Predator</td>\n",
       "      <td>[when a young boy accidentally triggers the un...</td>\n",
       "      <td>youll never see him coming.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Escape Room</td>\n",
       "      <td>[six strangers find themselves in a maze of de...</td>\n",
       "      <td>solve the puzzle. escape the room. find the cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Indiana Jones and the Last Crusade</td>\n",
       "      <td>[in  after his father professor henry jones sr...</td>\n",
       "      <td>have the adventure of your life keeping up wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jaws</td>\n",
       "      <td>[when a killer shark unleashes chaos on a beac...</td>\n",
       "      <td>there is a creature alive today which has surv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Life of Pi</td>\n",
       "      <td>[a young man who survives a disaster at sea is...</td>\n",
       "      <td>dont lose hope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>Should I Stay or Should I Go?</td>\n",
       "      <td>[people in five countries ask themselves the i...</td>\n",
       "      <td>a european exploration of the decisions in life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>Andrew and Jeremy Get Married</td>\n",
       "      <td>[an intimate portrait of two englishmen from v...</td>\n",
       "      <td>you may kiss the groom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>Touchstone: Dancing with Angels</td>\n",
       "      <td>[touchstone dancing with angels is the unforge...</td>\n",
       "      <td>discover the magic the mastery and the mystery...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22505</th>\n",
       "      <td>Souvenir</td>\n",
       "      <td>[on their last night in thailand danny jenkins...</td>\n",
       "      <td>danny brought home more than just the tshirt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22506</th>\n",
       "      <td>Station House</td>\n",
       "      <td>[a documentary film chronicling the reallife c...</td>\n",
       "      <td>real life. real danger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22507</th>\n",
       "      <td>A Second Chance: The story of D-backs' David P...</td>\n",
       "      <td>[a second chance  is a documentary about dback...</td>\n",
       "      <td>the story of dbacks david peralta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22508</th>\n",
       "      <td>Private Parking</td>\n",
       "      <td>[two lovers,  two secrets,  two timing,  someo...</td>\n",
       "      <td>whos watching you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22509</th>\n",
       "      <td>One Night in Portland</td>\n",
       "      <td>[when an agent of a secret international gover...</td>\n",
       "      <td>even if you dont believe in aliens they believ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22510</th>\n",
       "      <td>Galloping Dynamite</td>\n",
       "      <td>[when bob dillon finds gold reed kills him,  b...</td>\n",
       "      <td>dynamite in both hands crashing like lightning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22511</th>\n",
       "      <td>16 Heads and Counting</td>\n",
       "      <td>[everyday man john porter comes to believe tha...</td>\n",
       "      <td>a love story with teeth.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>Independent Intervention</td>\n",
       "      <td>[arguing for the need of an independent media ...</td>\n",
       "      <td>independent intervention is an awardwinning do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22513</th>\n",
       "      <td>Border Fence</td>\n",
       "      <td>[filmed in the san antonio area by h,  w,  kie...</td>\n",
       "      <td>get off my land...or get under it original poster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22514</th>\n",
       "      <td>Soldiers United for Cash</td>\n",
       "      <td>[a documentary detailing the origins of screws...</td>\n",
       "      <td>biography of dj screwthe screwed up click</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22515</th>\n",
       "      <td>Gods in Shackles</td>\n",
       "      <td>[gods in shackles is an expose revealing the d...</td>\n",
       "      <td>a documentary that lifts off the cultural veil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22516</th>\n",
       "      <td>Border City Rustlers</td>\n",
       "      <td>[two episodes of wild bill hickok edited toget...</td>\n",
       "      <td>trigger trap for renegades</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>Orange Inn</td>\n",
       "      <td>[it isnt the same the next time,  geo from gre...</td>\n",
       "      <td>longing taking chances and second chances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22518</th>\n",
       "      <td>Decadence: Decline of the Western World</td>\n",
       "      <td>[the goal of every culture is to decay through...</td>\n",
       "      <td>decadence asks a final dark age or a new renai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22519</th>\n",
       "      <td>Gridiron Girls</td>\n",
       "      <td>[explore the lives both on and off the footbal...</td>\n",
       "      <td>some women dread football these women live for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22520</th>\n",
       "      <td>My Annoying Dead Brother</td>\n",
       "      <td>[dr,  patrick ryan  is an organized by the boo...</td>\n",
       "      <td>you can be dead and still be a jerk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22521</th>\n",
       "      <td>The 'Yeah Whatever' Girl</td>\n",
       "      <td>[it is the eve of gay pride day in toronto as ...</td>\n",
       "      <td>if scrooge were a lesbian shed be the yeah wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22522</th>\n",
       "      <td>Breaking News: The Collision of Journalism and...</td>\n",
       "      <td>[in  hundreds of thousands of ukrainian citize...</td>\n",
       "      <td>using ukraine and the united states as example...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22523</th>\n",
       "      <td>The Highway Home</td>\n",
       "      <td>[the story begins at a pool hall where sixteen...</td>\n",
       "      <td>where would you be if what came before never h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22524</th>\n",
       "      <td>The Big Diamond Robbery</td>\n",
       "      <td>[tom markham is the foreman of an arizona dude...</td>\n",
       "      <td>notice as tom mix has joined the circus and no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22525</th>\n",
       "      <td>Lost Focus</td>\n",
       "      <td>[a famous fashion photographer is at the end o...</td>\n",
       "      <td>the lens of love is the most deadly when you l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22526</th>\n",
       "      <td>Second Hand Store Horror Tales</td>\n",
       "      <td>[second hand store a film of short horror tale...</td>\n",
       "      <td>buyer beware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22527</th>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>[shot in toronto badge of pride takes a look a...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22528</th>\n",
       "      <td>Beautiful But Dumb</td>\n",
       "      <td>[janet brady a stenographer wants the love of ...</td>\n",
       "      <td>she takes lessons from her smart girl friend i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22529</th>\n",
       "      <td>RODS: Mysterious Objects Among Us!</td>\n",
       "      <td>[video evidence of what some experts believe t...</td>\n",
       "      <td>they appear to be alive learn how to capture r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22530</th>\n",
       "      <td>A Broadway Scandal</td>\n",
       "      <td>[nenette bisson who dances in her fathers fren...</td>\n",
       "      <td>an amazing romance of a french girl in new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22531</th>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>[in the beginning of  the italian filmmaker ia...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22532 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              movie_name  \\\n",
       "0                                                  Joker   \n",
       "1                                      Rambo: Last Blood   \n",
       "2                                                 Stuber   \n",
       "3                      John Wick: Chapter 3 - Parabellum   \n",
       "4                                             Knives Out   \n",
       "5                                                  Crawl   \n",
       "6                           Maleficent: Mistress of Evil   \n",
       "7                                 The Godfather: Part II   \n",
       "8      The Lord of the Rings: The Fellowship of the Ring   \n",
       "9              Star Wars: Episode I - The Phantom Menace   \n",
       "10           Fantastic Beasts: The Crimes of Grindelwald   \n",
       "11                                                 Ghost   \n",
       "12                                         The Outsiders   \n",
       "13                                         The Aftermath   \n",
       "14                                    Kong: Skull Island   \n",
       "15                                             Cast Away   \n",
       "16               Fantastic Beasts and Where to Find Them   \n",
       "17                                             Time Trap   \n",
       "18                                            Goosebumps   \n",
       "19                                   Clash of the Titans   \n",
       "20                            The Huntsman: Winter's War   \n",
       "21                                            The Grinch   \n",
       "22                                                 Dumbo   \n",
       "23                                   Clash of the Titans   \n",
       "24                                               Spectre   \n",
       "25                                          The Predator   \n",
       "26                                           Escape Room   \n",
       "27                    Indiana Jones and the Last Crusade   \n",
       "28                                                  Jaws   \n",
       "29                                            Life of Pi   \n",
       "...                                                  ...   \n",
       "22502                      Should I Stay or Should I Go?   \n",
       "22503                      Andrew and Jeremy Get Married   \n",
       "22504                    Touchstone: Dancing with Angels   \n",
       "22505                                           Souvenir   \n",
       "22506                                      Station House   \n",
       "22507  A Second Chance: The story of D-backs' David P...   \n",
       "22508                                    Private Parking   \n",
       "22509                              One Night in Portland   \n",
       "22510                                 Galloping Dynamite   \n",
       "22511                              16 Heads and Counting   \n",
       "22512                           Independent Intervention   \n",
       "22513                                       Border Fence   \n",
       "22514                           Soldiers United for Cash   \n",
       "22515                                   Gods in Shackles   \n",
       "22516                               Border City Rustlers   \n",
       "22517                                         Orange Inn   \n",
       "22518            Decadence: Decline of the Western World   \n",
       "22519                                     Gridiron Girls   \n",
       "22520                           My Annoying Dead Brother   \n",
       "22521                           The 'Yeah Whatever' Girl   \n",
       "22522  Breaking News: The Collision of Journalism and...   \n",
       "22523                                   The Highway Home   \n",
       "22524                            The Big Diamond Robbery   \n",
       "22525                                         Lost Focus   \n",
       "22526                     Second Hand Store Horror Tales   \n",
       "22527                                     Badge of Pride   \n",
       "22528                                 Beautiful But Dumb   \n",
       "22529                 RODS: Mysterious Objects Among Us!   \n",
       "22530                                 A Broadway Scandal   \n",
       "22531                                  We Call It Skweee   \n",
       "\n",
       "                                           movie_summary  \\\n",
       "0      [in gotham city mentallytroubled comedian arth...   \n",
       "1      [rambo must confront his past and unearth his ...   \n",
       "2      [a detective recruits his uber driver into an ...   \n",
       "3      [john wick is on the run after killing a membe...   \n",
       "4      [a detective investigates the death of a patri...   \n",
       "5      [a young woman while attempting to save her fa...   \n",
       "6      [maleficent and her goddaughter aurora begin t...   \n",
       "7      [the early life and career of vito corleone in...   \n",
       "8      [a meek hobbit from the shire and eight compan...   \n",
       "9      [two jedi escape a hostile blockade to find al...   \n",
       "10     [the second installment of the fantastic beast...   \n",
       "11     [after a young man is murdered his spirit stay...   \n",
       "12     [the rivalry between two gangs the poor grease...   \n",
       "13     [post world war ii a british colonel and his w...   \n",
       "14     [after the vietnam war a team of scientists ex...   \n",
       "15     [a fedex executive undergoes a physical and em...   \n",
       "16     [the adventures of writer newt scamander in ne...   \n",
       "17     [a professor enters a cave and goes missing,  ...   \n",
       "18     [a teenager teams up with the daughter of youn...   \n",
       "19     [perseus must battle medusa and the kraken to ...   \n",
       "20     [eric and fellow warrior sara raised as member...   \n",
       "21     [a grumpy grinch plots to ruin christmas for t...   \n",
       "22     [a young elephant whose oversized ears enable ...   \n",
       "23     [perseus demigod son of zeus battles the minio...   \n",
       "24     [a cryptic message from s past sends him pitte...   \n",
       "25     [when a young boy accidentally triggers the un...   \n",
       "26     [six strangers find themselves in a maze of de...   \n",
       "27     [in  after his father professor henry jones sr...   \n",
       "28     [when a killer shark unleashes chaos on a beac...   \n",
       "29     [a young man who survives a disaster at sea is...   \n",
       "...                                                  ...   \n",
       "22502  [people in five countries ask themselves the i...   \n",
       "22503  [an intimate portrait of two englishmen from v...   \n",
       "22504  [touchstone dancing with angels is the unforge...   \n",
       "22505  [on their last night in thailand danny jenkins...   \n",
       "22506  [a documentary film chronicling the reallife c...   \n",
       "22507  [a second chance  is a documentary about dback...   \n",
       "22508  [two lovers,  two secrets,  two timing,  someo...   \n",
       "22509  [when an agent of a secret international gover...   \n",
       "22510  [when bob dillon finds gold reed kills him,  b...   \n",
       "22511  [everyday man john porter comes to believe tha...   \n",
       "22512  [arguing for the need of an independent media ...   \n",
       "22513  [filmed in the san antonio area by h,  w,  kie...   \n",
       "22514  [a documentary detailing the origins of screws...   \n",
       "22515  [gods in shackles is an expose revealing the d...   \n",
       "22516  [two episodes of wild bill hickok edited toget...   \n",
       "22517  [it isnt the same the next time,  geo from gre...   \n",
       "22518  [the goal of every culture is to decay through...   \n",
       "22519  [explore the lives both on and off the footbal...   \n",
       "22520  [dr,  patrick ryan  is an organized by the boo...   \n",
       "22521  [it is the eve of gay pride day in toronto as ...   \n",
       "22522  [in  hundreds of thousands of ukrainian citize...   \n",
       "22523  [the story begins at a pool hall where sixteen...   \n",
       "22524  [tom markham is the foreman of an arizona dude...   \n",
       "22525  [a famous fashion photographer is at the end o...   \n",
       "22526  [second hand store a film of short horror tale...   \n",
       "22527  [shot in toronto badge of pride takes a look a...   \n",
       "22528  [janet brady a stenographer wants the love of ...   \n",
       "22529  [video evidence of what some experts believe t...   \n",
       "22530  [nenette bisson who dances in her fathers fren...   \n",
       "22531  [in the beginning of  the italian filmmaker ia...   \n",
       "\n",
       "                                           movie_tagline  \n",
       "0                                   put on a happy face.  \n",
       "1              they drew first blood. he will draw last.  \n",
       "2                                    buckle up for aride  \n",
       "3                on may th every action has consequences  \n",
       "4              everyone has a motive. no one has a clue.  \n",
       "5                                   they were here first  \n",
       "6                    on octobergo beyond the fairy tale.  \n",
       "7            all the power on earth cant change destiny.  \n",
       "8      its power corrupts all who desire it. only one...  \n",
       "9                                    one truth one hate.  \n",
       "10        the fate of one will change the future of all.  \n",
       "11     before sam was murdered he told molly hed love...  \n",
       "12               s.e. hintons classic novel about youth.  \n",
       "13     in the aftermath of war the last thing she exp...  \n",
       "14                                     all hail the king  \n",
       "15          at the edge of the world his journey begins.  \n",
       "16              what would you do if your beasts escaped  \n",
       "17                                 escape to the future.  \n",
       "18                          you will believe in monsters  \n",
       "19                              experience the fantastic  \n",
       "20                 discover the story before snow white.  \n",
       "21                                christmas is canceled.  \n",
       "22        ina beloved tale will take you to new heights.  \n",
       "23                                 the clash begins in d  \n",
       "24                                    the dead are alive  \n",
       "25                           youll never see him coming.  \n",
       "26     solve the puzzle. escape the room. find the cl...  \n",
       "27     have the adventure of your life keeping up wit...  \n",
       "28     there is a creature alive today which has surv...  \n",
       "29                                       dont lose hope.  \n",
       "...                                                  ...  \n",
       "22502   a european exploration of the decisions in life.  \n",
       "22503                            you may kiss the groom.  \n",
       "22504  discover the magic the mastery and the mystery...  \n",
       "22505      danny brought home more than just the tshirt.  \n",
       "22506                             real life. real danger  \n",
       "22507                  the story of dbacks david peralta  \n",
       "22508                                  whos watching you  \n",
       "22509  even if you dont believe in aliens they believ...  \n",
       "22510  dynamite in both hands crashing like lightning...  \n",
       "22511                           a love story with teeth.  \n",
       "22512  independent intervention is an awardwinning do...  \n",
       "22513  get off my land...or get under it original poster  \n",
       "22514          biography of dj screwthe screwed up click  \n",
       "22515  a documentary that lifts off the cultural veil...  \n",
       "22516                         trigger trap for renegades  \n",
       "22517          longing taking chances and second chances  \n",
       "22518  decadence asks a final dark age or a new renai...  \n",
       "22519  some women dread football these women live for...  \n",
       "22520                you can be dead and still be a jerk  \n",
       "22521  if scrooge were a lesbian shed be the yeah wha...  \n",
       "22522  using ukraine and the united states as example...  \n",
       "22523  where would you be if what came before never h...  \n",
       "22524  notice as tom mix has joined the circus and no...  \n",
       "22525  the lens of love is the most deadly when you l...  \n",
       "22526                                       buyer beware  \n",
       "22527      will the force still be with you if youre gay  \n",
       "22528  she takes lessons from her smart girl friend i...  \n",
       "22529  they appear to be alive learn how to capture r...  \n",
       "22530    an amazing romance of a french girl in new york  \n",
       "22531   a documentary about music people and scandinavia  \n",
       "\n",
       "[22532 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = preprocessed_df(df)\n",
    "dataframe[['movie_name', 'movie_summary', 'movie_tagline']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_summary</th>\n",
       "      <th>movie_tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Joker</td>\n",
       "      <td>in gotham city mentallytroubled comedian arthu...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Joker</td>\n",
       "      <td>he then embarks on a downward spiral of revol...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Joker</td>\n",
       "      <td>this path brings him facetoface with his alte...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Joker</td>\n",
       "      <td>joker centers around an origin of the iconic ...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Joker</td>\n",
       "      <td>todd phillips exploration of arthur fleck joa...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Joker</td>\n",
       "      <td>arthur fleck works as a clown and is an aspir...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Joker</td>\n",
       "      <td>he has mental health issues part of which inv...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Joker</td>\n",
       "      <td>times are tough and due to his issues and occ...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Joker</td>\n",
       "      <td>over time these issues bear down on him shapi...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>Joker</td>\n",
       "      <td>joker</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>Joker</td>\n",
       "      <td>arthur fleck is a wannabe standup comic who s...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>Joker</td>\n",
       "      <td>arthurs mental health causes almost all peopl...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>Joker</td>\n",
       "      <td>after being brutally beaten having his medica...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>Joker</td>\n",
       "      <td>a socially inept clown for hire  arthur fleck...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>Joker</td>\n",
       "      <td>he takes care of his mother penny fleck and a...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>Joker</td>\n",
       "      <td>dealing with all the negativity and bullying ...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>Joker</td>\n",
       "      <td>arthur fleck is a poor middle age man with me...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>Joker</td>\n",
       "      <td>he shows just how bad times can get as he sho...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>Joker</td>\n",
       "      <td>he has no one meaning he has nothing the help...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>Joker</td>\n",
       "      <td>struggling to make people laugh in grim early...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>Joker</td>\n",
       "      <td>mocked bullied and above all marginalised fle...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>Joker</td>\n",
       "      <td>then the medications stopped working and trou...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>Joker</td>\n",
       "      <td>is the world prepared for the gloriously male...</td>\n",
       "      <td>put on a happy face.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>Rambo: Last Blood</td>\n",
       "      <td>rambo must confront his past and unearth his r...</td>\n",
       "      <td>they drew first blood. he will draw last.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>Rambo: Last Blood</td>\n",
       "      <td>almost four decades after they drew first blo...</td>\n",
       "      <td>they drew first blood. he will draw last.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>Rambo: Last Blood</td>\n",
       "      <td>now rambo must confront his past and unearth ...</td>\n",
       "      <td>they drew first blood. he will draw last.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>Rambo: Last Blood</td>\n",
       "      <td>a deadly journey of vengeance rambo last bloo...</td>\n",
       "      <td>they drew first blood. he will draw last.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30</td>\n",
       "      <td>Stuber</td>\n",
       "      <td>a detective recruits his uber driver into an u...</td>\n",
       "      <td>buckle up for aride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31</td>\n",
       "      <td>Stuber</td>\n",
       "      <td>a mildmannered uber driver named stu picks up...</td>\n",
       "      <td>buckle up for aride</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>33</td>\n",
       "      <td>John Wick: Chapter 3 - Parabellum</td>\n",
       "      <td>john wick is on the run after killing a member...</td>\n",
       "      <td>on may th every action has consequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158386</th>\n",
       "      <td>184445</td>\n",
       "      <td>Second Hand Store Horror Tales</td>\n",
       "      <td>second hand store a film of short horror tales</td>\n",
       "      <td>buyer beware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158387</th>\n",
       "      <td>184447</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>shot in toronto badge of pride takes a look at...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158388</th>\n",
       "      <td>184448</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>four gay cops speak out</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158389</th>\n",
       "      <td>184449</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>for each being gay has shaped their professio...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158390</th>\n",
       "      <td>184450</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>two veterans a rookie and a copper who was so...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158391</th>\n",
       "      <td>184451</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>the stories provide a candid and unflinching ...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158392</th>\n",
       "      <td>184452</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>featured are constable jackie okeefe  a ten y...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158393</th>\n",
       "      <td>184453</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>badge of pride is a documentary that looks at...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158394</th>\n",
       "      <td>184454</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>over the course of one year starting and endi...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158395</th>\n",
       "      <td>184455</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>coming out as a gay cop has its price</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158396</th>\n",
       "      <td>184456</td>\n",
       "      <td>Badge of Pride</td>\n",
       "      <td>badge of pride will ask will the force be wit...</td>\n",
       "      <td>will the force still be with you if youre gay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158397</th>\n",
       "      <td>184457</td>\n",
       "      <td>Beautiful But Dumb</td>\n",
       "      <td>janet brady a stenographer wants the love of h...</td>\n",
       "      <td>she takes lessons from her smart girl friend i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158398</th>\n",
       "      <td>184458</td>\n",
       "      <td>Beautiful But Dumb</td>\n",
       "      <td>she learns her personality has no appeal so s...</td>\n",
       "      <td>she takes lessons from her smart girl friend i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158399</th>\n",
       "      <td>184459</td>\n",
       "      <td>Beautiful But Dumb</td>\n",
       "      <td>she develops sex appeal plus and though her b...</td>\n",
       "      <td>she takes lessons from her smart girl friend i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158400</th>\n",
       "      <td>184461</td>\n",
       "      <td>RODS: Mysterious Objects Among Us!</td>\n",
       "      <td>video evidence of what some experts believe to...</td>\n",
       "      <td>they appear to be alive learn how to capture r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158401</th>\n",
       "      <td>184462</td>\n",
       "      <td>RODS: Mysterious Objects Among Us!</td>\n",
       "      <td>cameraman and filmaker jose escamilla with hi...</td>\n",
       "      <td>they appear to be alive learn how to capture r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158402</th>\n",
       "      <td>184463</td>\n",
       "      <td>RODS: Mysterious Objects Among Us!</td>\n",
       "      <td>this video documents their journey</td>\n",
       "      <td>they appear to be alive learn how to capture r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158403</th>\n",
       "      <td>184464</td>\n",
       "      <td>RODS: Mysterious Objects Among Us!</td>\n",
       "      <td>for the first time you will see remarkable da...</td>\n",
       "      <td>they appear to be alive learn how to capture r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158404</th>\n",
       "      <td>184465</td>\n",
       "      <td>RODS: Mysterious Objects Among Us!</td>\n",
       "      <td>testimony by renowned ufo researchers corrobo...</td>\n",
       "      <td>they appear to be alive learn how to capture r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158405</th>\n",
       "      <td>184467</td>\n",
       "      <td>A Broadway Scandal</td>\n",
       "      <td>nenette bisson who dances in her fathers frenc...</td>\n",
       "      <td>an amazing romance of a french girl in new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158406</th>\n",
       "      <td>184468</td>\n",
       "      <td>A Broadway Scandal</td>\n",
       "      <td>the driver leaves her at the hospital of davi...</td>\n",
       "      <td>an amazing romance of a french girl in new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158407</th>\n",
       "      <td>184469</td>\n",
       "      <td>A Broadway Scandal</td>\n",
       "      <td>nenettes parents turn her out when they learn...</td>\n",
       "      <td>an amazing romance of a french girl in new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158408</th>\n",
       "      <td>184470</td>\n",
       "      <td>A Broadway Scandal</td>\n",
       "      <td>david serves overseas for two years during wo...</td>\n",
       "      <td>an amazing romance of a french girl in new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158409</th>\n",
       "      <td>184471</td>\n",
       "      <td>A Broadway Scandal</td>\n",
       "      <td>on his return he proclaims his love for nenet...</td>\n",
       "      <td>an amazing romance of a french girl in new york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158410</th>\n",
       "      <td>184473</td>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>in the beginning of  the italian filmmaker iac...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158411</th>\n",
       "      <td>184474</td>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>during his stay in sweden he got in touch wit...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158412</th>\n",
       "      <td>184475</td>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>active in the neapolitan dubstep scene iacopo...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158413</th>\n",
       "      <td>184476</td>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>he decided to follow some of the central skwe...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158414</th>\n",
       "      <td>184477</td>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>the trip went from the stockholm suburbs over...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158415</th>\n",
       "      <td>184478</td>\n",
       "      <td>We Call It Skweee</td>\n",
       "      <td>we call it skweee is a foreigners view on a c...</td>\n",
       "      <td>a documentary about music people and scandinavia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158416 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                          movie_name  \\\n",
       "0            0                               Joker   \n",
       "1            1                               Joker   \n",
       "2            2                               Joker   \n",
       "3            3                               Joker   \n",
       "4            4                               Joker   \n",
       "5            5                               Joker   \n",
       "6            6                               Joker   \n",
       "7            7                               Joker   \n",
       "8            8                               Joker   \n",
       "9           11                               Joker   \n",
       "10          12                               Joker   \n",
       "11          13                               Joker   \n",
       "12          14                               Joker   \n",
       "13          15                               Joker   \n",
       "14          16                               Joker   \n",
       "15          17                               Joker   \n",
       "16          18                               Joker   \n",
       "17          19                               Joker   \n",
       "18          20                               Joker   \n",
       "19          21                               Joker   \n",
       "20          22                               Joker   \n",
       "21          23                               Joker   \n",
       "22          24                               Joker   \n",
       "23          25                   Rambo: Last Blood   \n",
       "24          26                   Rambo: Last Blood   \n",
       "25          27                   Rambo: Last Blood   \n",
       "26          28                   Rambo: Last Blood   \n",
       "27          30                              Stuber   \n",
       "28          31                              Stuber   \n",
       "29          33   John Wick: Chapter 3 - Parabellum   \n",
       "...        ...                                 ...   \n",
       "158386  184445      Second Hand Store Horror Tales   \n",
       "158387  184447                      Badge of Pride   \n",
       "158388  184448                      Badge of Pride   \n",
       "158389  184449                      Badge of Pride   \n",
       "158390  184450                      Badge of Pride   \n",
       "158391  184451                      Badge of Pride   \n",
       "158392  184452                      Badge of Pride   \n",
       "158393  184453                      Badge of Pride   \n",
       "158394  184454                      Badge of Pride   \n",
       "158395  184455                      Badge of Pride   \n",
       "158396  184456                      Badge of Pride   \n",
       "158397  184457                  Beautiful But Dumb   \n",
       "158398  184458                  Beautiful But Dumb   \n",
       "158399  184459                  Beautiful But Dumb   \n",
       "158400  184461  RODS: Mysterious Objects Among Us!   \n",
       "158401  184462  RODS: Mysterious Objects Among Us!   \n",
       "158402  184463  RODS: Mysterious Objects Among Us!   \n",
       "158403  184464  RODS: Mysterious Objects Among Us!   \n",
       "158404  184465  RODS: Mysterious Objects Among Us!   \n",
       "158405  184467                  A Broadway Scandal   \n",
       "158406  184468                  A Broadway Scandal   \n",
       "158407  184469                  A Broadway Scandal   \n",
       "158408  184470                  A Broadway Scandal   \n",
       "158409  184471                  A Broadway Scandal   \n",
       "158410  184473                   We Call It Skweee   \n",
       "158411  184474                   We Call It Skweee   \n",
       "158412  184475                   We Call It Skweee   \n",
       "158413  184476                   We Call It Skweee   \n",
       "158414  184477                   We Call It Skweee   \n",
       "158415  184478                   We Call It Skweee   \n",
       "\n",
       "                                            movie_summary  \\\n",
       "0       in gotham city mentallytroubled comedian arthu...   \n",
       "1        he then embarks on a downward spiral of revol...   \n",
       "2        this path brings him facetoface with his alte...   \n",
       "3        joker centers around an origin of the iconic ...   \n",
       "4        todd phillips exploration of arthur fleck joa...   \n",
       "5        arthur fleck works as a clown and is an aspir...   \n",
       "6        he has mental health issues part of which inv...   \n",
       "7        times are tough and due to his issues and occ...   \n",
       "8        over time these issues bear down on him shapi...   \n",
       "9                                                   joker   \n",
       "10       arthur fleck is a wannabe standup comic who s...   \n",
       "11       arthurs mental health causes almost all peopl...   \n",
       "12       after being brutally beaten having his medica...   \n",
       "13       a socially inept clown for hire  arthur fleck...   \n",
       "14       he takes care of his mother penny fleck and a...   \n",
       "15       dealing with all the negativity and bullying ...   \n",
       "16       arthur fleck is a poor middle age man with me...   \n",
       "17       he shows just how bad times can get as he sho...   \n",
       "18       he has no one meaning he has nothing the help...   \n",
       "19       struggling to make people laugh in grim early...   \n",
       "20       mocked bullied and above all marginalised fle...   \n",
       "21       then the medications stopped working and trou...   \n",
       "22       is the world prepared for the gloriously male...   \n",
       "23      rambo must confront his past and unearth his r...   \n",
       "24       almost four decades after they drew first blo...   \n",
       "25       now rambo must confront his past and unearth ...   \n",
       "26       a deadly journey of vengeance rambo last bloo...   \n",
       "27      a detective recruits his uber driver into an u...   \n",
       "28       a mildmannered uber driver named stu picks up...   \n",
       "29      john wick is on the run after killing a member...   \n",
       "...                                                   ...   \n",
       "158386     second hand store a film of short horror tales   \n",
       "158387  shot in toronto badge of pride takes a look at...   \n",
       "158388                            four gay cops speak out   \n",
       "158389   for each being gay has shaped their professio...   \n",
       "158390   two veterans a rookie and a copper who was so...   \n",
       "158391   the stories provide a candid and unflinching ...   \n",
       "158392   featured are constable jackie okeefe  a ten y...   \n",
       "158393   badge of pride is a documentary that looks at...   \n",
       "158394   over the course of one year starting and endi...   \n",
       "158395              coming out as a gay cop has its price   \n",
       "158396   badge of pride will ask will the force be wit...   \n",
       "158397  janet brady a stenographer wants the love of h...   \n",
       "158398   she learns her personality has no appeal so s...   \n",
       "158399   she develops sex appeal plus and though her b...   \n",
       "158400  video evidence of what some experts believe to...   \n",
       "158401   cameraman and filmaker jose escamilla with hi...   \n",
       "158402                 this video documents their journey   \n",
       "158403   for the first time you will see remarkable da...   \n",
       "158404   testimony by renowned ufo researchers corrobo...   \n",
       "158405  nenette bisson who dances in her fathers frenc...   \n",
       "158406   the driver leaves her at the hospital of davi...   \n",
       "158407   nenettes parents turn her out when they learn...   \n",
       "158408   david serves overseas for two years during wo...   \n",
       "158409   on his return he proclaims his love for nenet...   \n",
       "158410  in the beginning of  the italian filmmaker iac...   \n",
       "158411   during his stay in sweden he got in touch wit...   \n",
       "158412   active in the neapolitan dubstep scene iacopo...   \n",
       "158413   he decided to follow some of the central skwe...   \n",
       "158414   the trip went from the stockholm suburbs over...   \n",
       "158415   we call it skweee is a foreigners view on a c...   \n",
       "\n",
       "                                            movie_tagline  \n",
       "0                                    put on a happy face.  \n",
       "1                                    put on a happy face.  \n",
       "2                                    put on a happy face.  \n",
       "3                                    put on a happy face.  \n",
       "4                                    put on a happy face.  \n",
       "5                                    put on a happy face.  \n",
       "6                                    put on a happy face.  \n",
       "7                                    put on a happy face.  \n",
       "8                                    put on a happy face.  \n",
       "9                                    put on a happy face.  \n",
       "10                                   put on a happy face.  \n",
       "11                                   put on a happy face.  \n",
       "12                                   put on a happy face.  \n",
       "13                                   put on a happy face.  \n",
       "14                                   put on a happy face.  \n",
       "15                                   put on a happy face.  \n",
       "16                                   put on a happy face.  \n",
       "17                                   put on a happy face.  \n",
       "18                                   put on a happy face.  \n",
       "19                                   put on a happy face.  \n",
       "20                                   put on a happy face.  \n",
       "21                                   put on a happy face.  \n",
       "22                                   put on a happy face.  \n",
       "23              they drew first blood. he will draw last.  \n",
       "24              they drew first blood. he will draw last.  \n",
       "25              they drew first blood. he will draw last.  \n",
       "26              they drew first blood. he will draw last.  \n",
       "27                                    buckle up for aride  \n",
       "28                                    buckle up for aride  \n",
       "29                on may th every action has consequences  \n",
       "...                                                   ...  \n",
       "158386                                       buyer beware  \n",
       "158387      will the force still be with you if youre gay  \n",
       "158388      will the force still be with you if youre gay  \n",
       "158389      will the force still be with you if youre gay  \n",
       "158390      will the force still be with you if youre gay  \n",
       "158391      will the force still be with you if youre gay  \n",
       "158392      will the force still be with you if youre gay  \n",
       "158393      will the force still be with you if youre gay  \n",
       "158394      will the force still be with you if youre gay  \n",
       "158395      will the force still be with you if youre gay  \n",
       "158396      will the force still be with you if youre gay  \n",
       "158397  she takes lessons from her smart girl friend i...  \n",
       "158398  she takes lessons from her smart girl friend i...  \n",
       "158399  she takes lessons from her smart girl friend i...  \n",
       "158400  they appear to be alive learn how to capture r...  \n",
       "158401  they appear to be alive learn how to capture r...  \n",
       "158402  they appear to be alive learn how to capture r...  \n",
       "158403  they appear to be alive learn how to capture r...  \n",
       "158404  they appear to be alive learn how to capture r...  \n",
       "158405    an amazing romance of a french girl in new york  \n",
       "158406    an amazing romance of a french girl in new york  \n",
       "158407    an amazing romance of a french girl in new york  \n",
       "158408    an amazing romance of a french girl in new york  \n",
       "158409    an amazing romance of a french girl in new york  \n",
       "158410   a documentary about music people and scandinavia  \n",
       "158411   a documentary about music people and scandinavia  \n",
       "158412   a documentary about music people and scandinavia  \n",
       "158413   a documentary about music people and scandinavia  \n",
       "158414   a documentary about music people and scandinavia  \n",
       "158415   a documentary about music people and scandinavia  \n",
       "\n",
       "[158416 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = new_dataframe(dataframe)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-ac475eba84dd>\", line 1, in <module>\n",
      "    dataframe4 = summary_split(df2)\n",
      "  File \"<ipython-input-11-25584dff1a13>\", line 19, in summary_split\n",
      "    df_split['split'][idx] = list(result)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 1041, in __setitem__\n",
      "    self._maybe_update_cacher()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 3139, in _maybe_update_cacher\n",
      "    ref._maybe_cache_changed(cacher[0], self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\", line 3096, in _maybe_cache_changed\n",
      "    self._data.set(item, value)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\", line 1062, in set\n",
      "    blknos = self._blknos[loc]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\contrib\\__init__.py\", line 52, in <module>\n",
      "    from tensorflow.contrib import graph_editor\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "dataframe4 = summary_split(df2)\n",
    "dataframe4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe4.to_json('moviedata_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe4 = pd.read_json('moviedata_last.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sources = [idx for idx in dataframe4['split']]\n",
    "targets = [i.split() for i in dataframe4['movie_tagline']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcumEZoyrseP"
   },
   "outputs": [],
   "source": [
    "\n",
    "# vocabulary for sources\n",
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab\n",
    "\n",
    "source2idx = {word : idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx : word for idx, word in enumerate(s_vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXB-VUBMrseS"
   },
   "outputs": [],
   "source": [
    "# vocabulary for targets\n",
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab\n",
    "\n",
    "\n",
    "target2idx = {word : idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx : word for idx, word in enumerate(t_vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_ = json.dumps(source2idx)\n",
    "f = open('source2idx.json', 'w')\n",
    "f.write(json_)\n",
    "f.close\n",
    "\n",
    "json_ = json.dumps(idx2source)\n",
    "f = open('idx2source.json', 'w')\n",
    "f.write(json_)\n",
    "f.close\n",
    "\n",
    "json_ = json.dumps(target2idx)\n",
    "f = open('target2idx.json', 'w')\n",
    "f.write(json_)\n",
    "f.close\n",
    "\n",
    "json_ = json.dumps(idx2target)\n",
    "f = open('idx2target.json', 'w')\n",
    "f.write(json_)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index화가 오래걸리기 때문에 dictionary를 json으로 저장하여 불러옴.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "source2idx = pd.read_json('source2idx.json', typ = 'series')\n",
    "idx2source = pd.read_json('idx2source.json', typ = 'series')\n",
    "target2idx = pd.read_json('target2idx.json', typ = 'series')\n",
    "idx2target = pd.read_json('idx2target.json', typ = 'series')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41vsoKVMrseU"
   },
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode = 'source'):\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source':\n",
    "        # preprocessing for source (encoder)\n",
    "        #각 summary에 있는 단어들을 인덱스로 변환.\n",
    "        s_input = list(map(lambda sentence : [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence : len(sentence), s_input))\n",
    "        #길이가 같지 않은 데이터들을 일정한 길이로 맞춰 준다.\n",
    "        #padding ='post' -> 각 시퀀스 뒤쪽으로 패딩한다\n",
    "        #maxlen -> 최대 길이를 max_len으로 설정하고 초과한 경우 각 시퀀스의 뒷쪽에서 자른다.\n",
    "        #summary별 단어 수가 다를것. 이를 맞춰주는 역할이 pad_sequences. 단어 수가 max_len보다 적거나 클경우\n",
    "        #뒷쪽부터 차이만큼 제거\n",
    "        s_input = pad_sequences(sequences = s_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target':\n",
    "        # preprocessing for target (decoder)\n",
    "        # 각 태그라인 앞 뒤에 <bos>와 <eos>를 추가\n",
    "        t_input = list(map(lambda sentence : ['<bos>'] + sentence + ['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence : [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence : len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences = t_input, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "         \n",
    "        # output\n",
    "        t_output = list(map(lambda sentence : sentence + ['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence : [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences = t_output, maxlen = max_len, padding = 'post', truncating = 'post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "va-4ia1Cr0O_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pnjqa9GOrseW"
   },
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for i in dataframe4.movie_summary:\n",
    "    length = len(i.split())\n",
    "    sum_ += length\n",
    "average_no = sum_ / len(dataframe4) #movie_summary당 평균 21.00 단어\n",
    "\n",
    "#단어 수 21개로 제한\n",
    "s_max_len = int(average_no)\n",
    "s_len, s_input = preprocess(sequences = sources,\n",
    "                            max_len = s_max_len, dic = source2idx, mode = 'source')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1c38GXgUrseY"
   },
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for i in dataframe4.movie_tagline:\n",
    "    length = len(i.split())\n",
    "    sum_ += length\n",
    "average_no = sum_ / len(dataframe4) #각 tagline별 단어 수는 9.6개\n",
    "\n",
    "#단어 수 9개로 제한\n",
    "t_max_len = int(average_no)\n",
    "t_len, t_input, t_output = preprocess(sequences = targets,\n",
    "                                      max_len = t_max_len, dic = target2idx, mode = 'target')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KfNB1D9nrseb"
   },
   "source": [
    "# hyper-param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDsB9Jm-rseb"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .005\n",
    "total_step = epochs / batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 256\n",
    "\n",
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size = buffer_size)\n",
    "data = data.batch(batch_size = batch_size, drop_remainder = True)\n",
    "# s_mb_len, s_mb_input, t_mb_len, t_mb_input, t_mb_output = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zeZsExSrsee"
   },
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True, \n",
    "                                        return_state=True, \n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True, \n",
    "                                   recurrent_activation='sigmoid', \n",
    "                                   recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjZ-wsk9rseg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "#         print(\"state: {}\".format(state.shape))\n",
    "#         print(\"output: {}\".format(state.shape))\n",
    "              \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1qCbpeersei"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # * `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "                \n",
    "        #* `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # * `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        # * `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        # * `merged vector = concat(embedding output, context vector)`\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buk-GeAVrsek"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "#     print(\"real: {}\".format(real))\n",
    "#     print(\"pred: {}\".format(pred))\n",
    "#     print(\"mask: {}\".format(mask))\n",
    "#     print(\"loss: {}\".format(tf.reduce_mean(loss_)))\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# creating optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# creating check point (Object-based saving)\n",
    "checkpoint_dir = './data_out/training_checkpoints_attention_after_changed_inputmax'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder)\n",
    "\n",
    "# create writer for tensorboard\n",
    "summary_writer = tf.contrib.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bS2gj0Oirsem"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 13339.0215 Batch Loss 6.0202\n",
      "Time taken for 1 epoch 2880.718366622925 sec\n",
      "\n",
      "Epoch 2 Loss 10353.8838 Batch Loss 4.8213\n",
      "Time taken for 1 epoch 2872.3880887031555 sec\n",
      "\n",
      "Epoch 4 Loss 8667.5342 Batch Loss 4.1176\n",
      "Time taken for 1 epoch 2875.099446296692 sec\n",
      "\n",
      "Epoch 6 Loss 7629.8569 Batch Loss 3.7809\n",
      "Time taken for 1 epoch 2874.1173901557922 sec\n",
      "\n",
      "Epoch 8 Loss 6770.5801 Batch Loss 3.3888\n",
      "Time taken for 1 epoch 2871.1832222938538 sec\n",
      "\n",
      "Epoch 10 Loss 6286.0312 Batch Loss 3.1381\n",
      "Time taken for 1 epoch 2871.9042637348175 sec\n",
      "\n",
      "Epoch 12 Loss 6041.6924 Batch Loss 3.2225\n",
      "Time taken for 1 epoch 2871.7372539043427 sec\n",
      "\n",
      "Epoch 14 Loss 5394.2275 Batch Loss 2.7991\n",
      "Time taken for 1 epoch 2871.6472487449646 sec\n",
      "\n",
      "Epoch 16 Loss 5230.9321 Batch Loss 2.6719\n",
      "Time taken for 1 epoch 2873.8093724250793 sec\n",
      "\n",
      "Epoch 18 Loss 4871.4990 Batch Loss 2.4849\n",
      "Time taken for 1 epoch 2870.6091895103455 sec\n",
      "\n",
      "Epoch 20 Loss 4905.0137 Batch Loss 2.5780\n",
      "Time taken for 1 epoch 2874.1173901557922 sec\n",
      "\n",
      "Epoch 22 Loss 4576.4819 Batch Loss 2.3048\n",
      "Time taken for 1 epoch 2873.101331949234 sec\n",
      "\n",
      "Epoch 24 Loss 5002.4331 Batch Loss 2.3940\n",
      "Time taken for 1 epoch 2872.1102755069733 sec\n",
      "\n",
      "Epoch 26 Loss 4825.0103 Batch Loss 2.3283\n",
      "Time taken for 1 epoch 2873.3043434619904 sec\n",
      "\n",
      "Epoch 28 Loss 4575.7500 Batch Loss 2.2251\n",
      "Time taken for 1 epoch 2866.2109375 sec\n",
      "\n",
      "Epoch 30 Loss 4316.9097 Batch Loss 2.0466\n",
      "Time taken for 1 epoch 2864.5178411006927 sec\n",
      "\n",
      "Epoch 32 Loss 4877.8413 Batch Loss 2.5794\n",
      "Time taken for 1 epoch 2865.1448769569397 sec\n",
      "\n",
      "Epoch 34 Loss 5019.2363 Batch Loss 2.8319\n",
      "Time taken for 1 epoch 2865.743708372116 sec\n",
      "\n",
      "Epoch 36 Loss 5638.3125 Batch Loss 2.6696\n",
      "Time taken for 1 epoch 2864.754854440689 sec\n",
      "\n",
      "Epoch 38 Loss 5308.3101 Batch Loss 2.6202\n",
      "Time taken for 1 epoch 2864.8248586654663 sec\n",
      "\n",
      "Epoch 40 Loss 5431.3408 Batch Loss 2.6177\n",
      "Time taken for 1 epoch 2865.123875617981 sec\n",
      "\n",
      "Epoch 42 Loss 5068.8540 Batch Loss 2.5526\n",
      "Time taken for 1 epoch 2865.848917245865 sec\n",
      "\n",
      "Epoch 44 Loss 4805.8589 Batch Loss 2.4658\n",
      "Time taken for 1 epoch 2866.2099375724792 sec\n",
      "\n",
      "Epoch 46 Loss 4654.8267 Batch Loss 2.4300\n",
      "Time taken for 1 epoch 2865.331887483597 sec\n",
      "\n",
      "Epoch 48 Loss 4501.9502 Batch Loss 2.2496\n",
      "Time taken for 1 epoch 2868.1250474452972 sec\n",
      "\n",
      "Epoch 50 Loss 4510.4985 Batch Loss 2.2436\n",
      "Time taken for 1 epoch 2868.4410655498505 sec\n",
      "\n",
      "Epoch 52 Loss 4430.3154 Batch Loss 2.1894\n",
      "Time taken for 1 epoch 2865.2348821163177 sec\n",
      "\n",
      "Epoch 54 Loss 4181.6855 Batch Loss 2.0258\n",
      "Time taken for 1 epoch 2866.3909480571747 sec\n",
      "\n",
      "Epoch 56 Loss 4228.2661 Batch Loss 2.0224\n",
      "Time taken for 1 epoch 2866.495954275131 sec\n",
      "\n",
      "Epoch 58 Loss 4034.9487 Batch Loss 1.9424\n",
      "Time taken for 1 epoch 2866.583959341049 sec\n",
      "\n",
      "Epoch 60 Loss 4104.8677 Batch Loss 2.0040\n",
      "Time taken for 1 epoch 2867.4220073223114 sec\n",
      "\n",
      "Epoch 62 Loss 4458.4683 Batch Loss 2.1158\n",
      "Time taken for 1 epoch 2866.6949656009674 sec\n",
      "\n",
      "Epoch 64 Loss 4339.1299 Batch Loss 2.0767\n",
      "Time taken for 1 epoch 2866.2859420776367 sec\n",
      "\n",
      "Epoch 66 Loss 4195.9819 Batch Loss 1.9414\n",
      "Time taken for 1 epoch 2868.268055200577 sec\n",
      "\n",
      "Epoch 68 Loss 4043.3921 Batch Loss 1.8864\n",
      "Time taken for 1 epoch 2867.4200072288513 sec\n",
      "\n",
      "Epoch 70 Loss 4025.7729 Batch Loss 1.9623\n",
      "Time taken for 1 epoch 2867.4870109558105 sec\n",
      "\n",
      "Epoch 72 Loss 4374.7256 Batch Loss 1.9841\n",
      "Time taken for 1 epoch 2867.599017381668 sec\n",
      "\n",
      "Epoch 74 Loss 3971.9731 Batch Loss 1.8641\n",
      "Time taken for 1 epoch 2867.761026620865 sec\n",
      "\n",
      "Epoch 76 Loss 3870.3508 Batch Loss 1.8424\n",
      "Time taken for 1 epoch 2866.899977207184 sec\n",
      "\n",
      "Epoch 78 Loss 3999.3054 Batch Loss 1.8502\n",
      "Time taken for 1 epoch 2870.9582092761993 sec\n",
      "\n",
      "Epoch 80 Loss 4187.4302 Batch Loss 1.8950\n",
      "Time taken for 1 epoch 2866.825973033905 sec\n",
      "\n",
      "Epoch 82 Loss 3870.8667 Batch Loss 1.7751\n",
      "Time taken for 1 epoch 2869.4831252098083 sec\n",
      "\n",
      "Epoch 84 Loss 4083.0662 Batch Loss 1.8008\n",
      "Time taken for 1 epoch 2869.0981030464172 sec\n",
      "\n",
      "Epoch 86 Loss 4008.7825 Batch Loss 1.7375\n",
      "Time taken for 1 epoch 2867.1769931316376 sec\n",
      "\n",
      "Epoch 88 Loss 3933.0330 Batch Loss 1.8167\n",
      "Time taken for 1 epoch 2867.6200184822083 sec\n",
      "\n",
      "Epoch 90 Loss 3847.1123 Batch Loss 1.7319\n",
      "Time taken for 1 epoch 2866.139933824539 sec\n",
      "\n",
      "Epoch 92 Loss 3946.9304 Batch Loss 1.7954\n",
      "Time taken for 1 epoch 2820.170755147934 sec\n",
      "\n",
      "Epoch 94 Loss 3995.1555 Batch Loss 1.8944\n",
      "Time taken for 1 epoch 2820.316754579544 sec\n",
      "\n",
      "Epoch 96 Loss 3955.0784 Batch Loss 1.7947\n",
      "Time taken for 1 epoch 2820.1271533966064 sec\n",
      "\n",
      "Epoch 98 Loss 3962.3669 Batch Loss 1.8338\n",
      "Time taken for 1 epoch 2818.8997514247894 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:            \n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)            \n",
    "            dec_hidden = enc_hidden            \n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']] * batch_size, 1)            \n",
    "            #Teacher Forcing: feeding the target as the next input\n",
    "            for t in range(1, t_input.shape[1]):\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)                \n",
    "                loss += loss_function(t_input[:, t], predictions)            \n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1) #using teacher forcing\n",
    "                \n",
    "        batch_loss = (loss / int(t_input.shape[1]))        \n",
    "        total_loss += batch_loss        \n",
    "        variables = encoder.variables + decoder.variables        \n",
    "        gradient = tape.gradient(loss, variables)        \n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "        \n",
    "    if epoch % 2 == 0:\n",
    "        #save model every 10 epoch\n",
    "        print('Epoch {} Loss {:.4f} Batch Loss {:.4f}'.format(epoch,\n",
    "                                            total_loss / n_batch,\n",
    "                                            batch_loss.numpy()))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FpYBEm2orseo"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \n",
    "    inputs = []\n",
    "    x = sentence.split(' ')\n",
    "    #keyerror 처리\n",
    "    for i in x:\n",
    "        if not i in inp_lang.keys():\n",
    "            pass\n",
    "        else:\n",
    "            inputs.append(inp_lang[i])            \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)   \n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang['<bos>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += idx2target[predicted_id] + ' '\n",
    "        if idx2target.get(predicted_id) == '<eos>':\n",
    "            return result       \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result\n",
    "\n",
    "# result, sentence, attention_plot = evaluate(sentence, encoder, decoder, source2idx, target2idx,\n",
    "#                                             s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J_oZ50Qrset"
   },
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1Al997Hrsev"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2b3b3c18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#restore checkpoint\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c17958706bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget2idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_max_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_max_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10개 결과값 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auBLsZAKrsex",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596\n",
      "Predicted translation: a field trip straight to hear it... in the \n",
      "None\n",
      "1755\n",
      "Predicted translation: meet the funniest story of success. <eos> \n",
      "None\n",
      "1280\n",
      "Predicted translation: a onenight stand together <eos> \n",
      "None\n",
      "1941\n",
      "Predicted translation: a tragic decision <eos> \n",
      "None\n",
      "1485\n",
      "Predicted translation: a job so great suspense allcaps lobby cards. <eos> \n",
      "None\n",
      "1406\n",
      "Predicted translation: a personal story of dj screwthe screwed up click \n",
      "None\n",
      "1388\n",
      "Predicted translation: a tragic decision <eos> \n",
      "None\n",
      "381\n",
      "Predicted translation: justify your evil. <eos> \n",
      "None\n",
      "752\n",
      "Predicted translation: put on the pathway of the ultimate dead <eos> \n",
      "None\n",
      "2536\n",
      "Predicted translation: put on a happy face. <eos> \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sentence = ' '.join(df.movie_summary[1])\n",
    "    sentence = str(sentence)\n",
    "    s = random.randint(1, len(sentence))\n",
    "    print(s)\n",
    "    sentence = sentence[:-s]\n",
    "    print(translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원하는 것만 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: justify your own luck. <eos> \n",
      "None\n",
      "Predicted translation: a drama of purity burned <eos> \n",
      "None\n",
      "Predicted translation: a tragic decision <eos> \n",
      "None\n",
      "Predicted translation: unlock me. <eos> \n",
      "None\n",
      "Predicted translation: a tale of the bravest man cared too little \n",
      "None\n",
      "Predicted translation: the quest begins <eos> \n",
      "None\n",
      "Predicted translation: put on a happy face. <eos> \n",
      "None\n",
      "Predicted translation: crime didnt see him coming. <eos> \n",
      "None\n",
      "Predicted translation: fingerprints of doing nothing. <eos> \n",
      "None\n",
      "Predicted translation: justify your evil. <eos> \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# sentence = 'tensorflow is a framework for deep learning'\n",
    "#1721: a tragic decision, 450: a documentary diary, 63: a cancerous dark comedy, \n",
    "#128: a worldwide phenomenon continues, 292: feature of laughs\n",
    "# 381: justify your own luck\n",
    "# 2259: a drama of purity burned\n",
    "# 1818: a story of purity burned\n",
    "# 766: a saint has a happy face\n",
    "#2507: unlock me\n",
    "#2536: put on a happy face\n",
    "#418: the quest begins -> the comedy begins\n",
    "#2660: the worldwide phenomenon continues\n",
    "#52: crime didn't see him coming\n",
    "#735 fingerprins of doing nothing\n",
    "list_ = [670, 2259, 1721, 2507, 63, 418, 2536, 52, 735, 381]\n",
    "\n",
    "for i in list_:\n",
    "    sentence = ' '.join(df.movie_summary[1])\n",
    "    sentence = str(sentence)\n",
    "    s = random.randint(1, len(sentence))\n",
    "    sentence = sentence[:-i]\n",
    "\n",
    "    print(translate(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_genre</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_poster</th>\n",
       "      <th>movie_summary</th>\n",
       "      <th>movie_synopsis</th>\n",
       "      <th>movie_tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124561</th>\n",
       "      <td>[Action]</td>\n",
       "      <td>The Killing Hand</td>\n",
       "      <td>www.imdb.com/video/imdb/vi3990290969?playlistI...</td>\n",
       "      <td>[it is said that there is evil inherent in all...</td>\n",
       "      <td>[\\n                , \\n            ]</td>\n",
       "      <td>justify your evil.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movie_genre        movie_name  \\\n",
       "124561    [Action]  The Killing Hand   \n",
       "\n",
       "                                             movie_poster  \\\n",
       "124561  www.imdb.com/video/imdb/vi3990290969?playlistI...   \n",
       "\n",
       "                                            movie_summary  \\\n",
       "124561  [it is said that there is evil inherent in all...   \n",
       "\n",
       "                              movie_synopsis       movie_tagline  \n",
       "124561  [\\n                , \\n            ]  justify your evil.  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['movie_tagline'].str.contains('justify your')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab-12-6-seq-to-seq-with-attention-keras-eager.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
