{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import iotools\n",
    "import numpy as np\n",
    "import random\n",
    "from wiki_data_op import Preprocessor\n",
    "\n",
    "wikidata_dir = os.path.join('.', 'wikidata')\n",
    "wikidoc_dir = os.path.join(wikidata_dir, 'wikidocs')\n",
    "vocab_dir = os.path.join(wikidata_dir, 'vocab')\n",
    "vocab_lexeme_fn = os.path.join(vocab_dir, 'vocab.bin')\n",
    "vocab_strings_fn = os.path.join(vocab_dir, 'strings.json')\n",
    "wikidoc_fn_template = 'wikidoc-%08d'\n",
    "\n",
    "train_dir = os.path.join('.', 'wikitrain')\n",
    "\n",
    "wikidoc_fn_re = re.compile('^wikidoc-[0-9]{8}$')\n",
    "\n",
    "for path in [wikidata_dir, wikidoc_dir, vocab_dir]:\n",
    "  if not os.path.exists(path):\n",
    "      os.makedirs(path)\n",
    "\n",
    "__nlp__ = None\n",
    "def nlp():\n",
    "  global __nlp__\n",
    "  if __nlp__ is None:\n",
    "    en = spacy.load('en')\n",
    "    with open(vocab_strings_fn, 'r') as f:\n",
    "      en.vocab.strings.load(f)\n",
    "    # en.vocab.load_lexemes(vocab_lexeme_fn)\n",
    "    __nlp__ = en\n",
    "  return __nlp__\n",
    "\n",
    "files = [\n",
    "    os.path.join(wikidoc_dir, fn)\n",
    "    for fn in os.listdir(wikidoc_dir)\n",
    "    if wikidoc_fn_re.match(fn) is not None\n",
    "]\n",
    "train_files = files[:int(len(files)*0.85)]\n",
    "random.shuffle(train_files)\n",
    "\n",
    "dev_files = files[int(len(files)*0.85):int(len(files)*0.95)]\n",
    "test_files = files[int(len(files)*0.85):int(len(files)*0.95)]\n",
    "\n",
    "vocab_size = 100000\n",
    "\n",
    "def normalize_sequence(seq: np.ndarray, oov_token=2, reserved_tokens=3, max_vocab=vocab_size):\n",
    "  seq = seq + 3\n",
    "  seq[seq > max_vocab-1] = oov_token\n",
    "  return seq\n",
    "\n",
    "def _read(files, nlp_=None, epochs=1):\n",
    "  if nlp_ is None:\n",
    "    print('loading nlp object')\n",
    "    nlp_ = nlp()\n",
    "  l = len(files)\n",
    "  print('reading from %s files' % l)\n",
    "  preproc = Preprocessor(nlp_.vocab)\n",
    "  for i in range(epochs):\n",
    "    for j, fn in enumerate(files):\n",
    "      with iotools.BinarySequenceFile(fn, 'rb') as f:\n",
    "        for binstr in f:\n",
    "          yield i, j, l, fn, preproc.unpack(binstr)\n",
    "\n",
    "def read_trainset(epochs=1):\n",
    "  return _read(train_files, epochs=epochs)\n",
    "\n",
    "def read_devset(epochs=1):\n",
    "  return _read(dev_files, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
